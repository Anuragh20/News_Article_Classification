{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MultinomialNB_Class.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRiRFAxkhQP"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "import math\n",
        "import nltk\n",
        "import copy\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bggpimYKM3cD",
        "outputId": "00808347-f727-4a6d-80d0-6696b47e209a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKtSiGY9GqxX",
        "outputId": "80c0888a-42ff-4c1f-df22-686f97ae7bf8"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stops = set(stopwords.words('english'))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9WN6VFrF35l"
      },
      "source": [
        "train_link='/content/drive/MyDrive/Exalens/BBC News Train.csv'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLGWXLd2DFca"
      },
      "source": [
        "def data_loading(link):\n",
        "    data=[]\n",
        "    with open(link,mode= 'r') as file:\n",
        "        csv_file= csv.reader(file)\n",
        "\n",
        "        for lines in csv_file:\n",
        "            data.append(lines)\n",
        "\n",
        "    data.pop(0)\n",
        "    for i in range(len(data)):\n",
        "        data[i].pop(0)\n",
        "  \n",
        "    return data"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSEtpPcBDH7z"
      },
      "source": [
        "def data_labels(data):\n",
        "    labels=[]\n",
        "    for i in range(len(data)):\n",
        "        labels.append(data[i].pop(1))\n",
        "    return labels"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6tt9pHGtWo"
      },
      "source": [
        "train_data= data_loading(train_link)\n",
        "train_labels= data_labels(train_data)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzzeamOT1Cou"
      },
      "source": [
        "def data_cleanup(text):\n",
        "    l=[]\n",
        "    fin=[]\n",
        "    for word in text.split(' '):\n",
        "        if word not in stops:\n",
        "            l.append(word)\n",
        "    x= ' '.join(l) \n",
        "    x= x.encode('ascii', 'ignore').decode()\n",
        "    x= re.sub(r'https*\\S+', ' ', x)\n",
        "    x= re.sub(r'@\\S+', ' ', x)\n",
        "    x= re.sub(r'#\\S+', ' ', x)\n",
        "    x= re.sub(r'\\'\\w+', '', x)\n",
        "    x= re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
        "    x= re.sub(r'\\w*\\d+\\w*', '', x)\n",
        "    x= re.sub(r'\\s{2,}', ' ', x)\n",
        "    fin.append(x)\n",
        "    return fin"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfCOFCv5tyU0"
      },
      "source": [
        "##Change this cell to len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39Hksln10TfM"
      },
      "source": [
        "for i in range(len(train_data)):\n",
        "    train_data[i]= data_cleanup(train_data[i][0])\n",
        "    train_data[i]= train_data[i][0].split(' ')"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq9TpLZpYC-6"
      },
      "source": [
        "uniq_labels=[]\n",
        "for lab in train_labels:\n",
        "    if lab not in uniq_labels:\n",
        "        uniq_labels.append(lab)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ewJ5LssBnya"
      },
      "source": [
        "data_list=[]\n",
        "for u in uniq_labels:\n",
        "    l_list=[]\n",
        "    for i in range(len(train_data)):\n",
        "        if train_labels[i]== u:\n",
        "            l_list.append(train_data[i])\n",
        "    data_list.append(l_list)\n",
        "input_data= dict(zip(uniq_labels,data_list))   "
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srYebsXMDduf"
      },
      "source": [
        "test_link= '/content/drive/MyDrive/Exalens/BBC News Test.csv'"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrSCjqBmDfBm"
      },
      "source": [
        "test_data= data_loading(test_link)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS2xKru_DerF"
      },
      "source": [
        "for i in range(len(test_data)):\n",
        "    test_data[i]= data_cleanup(test_data[i][0])\n",
        "    test_data[i]= test_data[i][0].split(' ')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck12DqLuDdmk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBAd48dAN3uT"
      },
      "source": [
        "class MultinomialNB:\n",
        "    def __init__(self, articles_per_tag):\n",
        "        # Don't change the following two lines of code.\n",
        "        self.articles_per_tag = articles_per_tag  # See question prompt for details.\n",
        "        self.train()\n",
        "   \n",
        "    def train(self):\n",
        "        def get_unique_values(new_list):\n",
        "            unique = []\n",
        "            for ele in new_list:\n",
        "                if ele not in unique:\n",
        "                    unique.append(ele)\n",
        "            return unique\n",
        "\n",
        "        def word_counts(uni_wrds,docs):\n",
        "            word_counter=[]\n",
        "            for u in range(len(uni_wrds)):\n",
        "                cnt=0\n",
        "                for doc in range(len(docs)):\n",
        "                    w_cnts= Counter(docs[doc])\n",
        "                    cnt= cnt + w_cnts.get(uni_wrds[u],0)\n",
        "                word_counter.append(cnt)\n",
        "            return word_counter\n",
        "\n",
        "        uniq_labels= self.articles_per_tag.keys()\n",
        "        uniq_list=[]\n",
        "        lab_wise_cnts=[]\n",
        "\n",
        "        for ul in uniq_labels:\n",
        "            lab_uniq_list=[]\n",
        "            lab_data= self.articles_per_tag.get(ul)\n",
        "            lc=0\n",
        "            for d1 in range(len(lab_data)):\n",
        "                word_list=get_unique_values(lab_data[d1])\n",
        "                lab_uniq_list.extend(word_list)\n",
        "                lc= lc+len(lab_data[d1])\n",
        "            uniq_list.extend(lab_uniq_list)\n",
        "            lab_wise_cnts.append(lc)\n",
        "        uniq_list= get_unique_values(uniq_list) \n",
        "    \n",
        "        lab_docword_counts=[]\n",
        "        for doc_lab in uniq_labels:\n",
        "            doc_list= self.articles_per_tag.get(doc_lab)\n",
        "            dwc_dict= dict(zip(uniq_list,word_counts(uniq_list,doc_list)))\n",
        "            lab_docword_counts.append(dwc_dict)\n",
        "        \n",
        "        self.train_wrd_cnt= dict(zip(uniq_labels,lab_docword_counts))\n",
        "        self.train_lab_cnt= dict(zip(uniq_labels,lab_wise_cnts))      \n",
        "        pass\n",
        "    \n",
        "    def predict(self, article):\n",
        "        def probability(word, w_dict):\n",
        "            prob= math.log((w_dict.get(word,0)+1)/(sum(w_dict.values())+2))  \n",
        "            return prob\n",
        "\n",
        "        uniq_labels= self.train_lab_cnt.keys()\n",
        "        res=[]\n",
        "        for d in range(len(article)):\n",
        "            interim_res=[]\n",
        "            for label in uniq_labels:\n",
        "                p=0\n",
        "                for k in range(len(article[d])):\n",
        "                    p= p + probability(article[d][k],self.train_wrd_cnt.get(label))\n",
        "                fin_p= round(p + math.log(self.train_lab_cnt.get(label)/sum(self.train_lab_cnt.values())),4)\n",
        "            \n",
        "                interim_res.append(fin_p)\n",
        "            res.append(interim_res)\n",
        "    \n",
        "        results_dict=[]\n",
        "        for r in range(len(res)):\n",
        "            d= dict(zip(uniq_labels,res[r]))\n",
        "            results_dict.append(d)\n",
        "\n",
        "        return results_dict\n",
        "        pass"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnv_ep1w5exG"
      },
      "source": [
        "mnb= MultinomialNB(input_data)\n",
        "mnb.train()"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smjm8C01B0t_",
        "outputId": "3217a13e-c026-4703-8432-415525357be4"
      },
      "source": [
        "new_test_results= mnb.predict(test_data)\n",
        "print(len(new_test_results))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNmmoe_BB0qn",
        "outputId": "8d478b18-ae8a-4101-e728-bd6218a0da24"
      },
      "source": [
        "print(new_test_results[:5])"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'business': -817.3246, 'tech': -843.3445, 'politics': -832.0478, 'sport': -746.1193, 'entertainment': -796.42}, {'business': -2120.0977, 'tech': -1934.0041, 'politics': -2136.9598, 'sport': -2283.8258, 'entertainment': -2215.7544}, {'business': -1301.0164, 'tech': -1316.4071, 'politics': -1281.1263, 'sport': -1063.794, 'entertainment': -1250.2868}, {'business': -1499.1667, 'tech': -1636.326, 'politics': -1585.5, 'sport': -1619.7699, 'entertainment': -1582.505}, {'business': -704.9955, 'tech': -729.4351, 'politics': -704.9155, 'sport': -623.2346, 'entertainment': -685.511}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5VM6UO1DvVZ"
      },
      "source": [
        "max_key=[]\n",
        "for n in range(len(new_test_results)):\n",
        "    max_key.append(max(new_test_results[n], key=lambda key: new_test_results[n][key]))"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkHxZDVVjfxa",
        "outputId": "b9df6ef2-298e-4d2d-c7d0-ad9af9690d55"
      },
      "source": [
        "max_key[:10]"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sport',\n",
              " 'tech',\n",
              " 'sport',\n",
              " 'business',\n",
              " 'sport',\n",
              " 'sport',\n",
              " 'politics',\n",
              " 'politics',\n",
              " 'entertainment',\n",
              " 'business']"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQymLkFajfsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZLyc81H7KZT"
      },
      "source": [
        "samp_data={\"politics\":[['articles','writes','joel','furr','writes',],['distribution','world','following','posted']], \"sports\":[['article','writes','just','wanted'],['phillies','salvaged','their','weekend']], \"tech\":[['thanks','steve','your','helpful'],['please','unsubscribe','this','user']]}"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhlQ9nFahACS"
      },
      "source": [
        "mnb= MultinomialNB(samp_data)\n",
        "mnb.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxmBOefxg_8h"
      },
      "source": [
        "art= ['article','writes','while','when','owned','plus','wanted','upgrade','memory','just','ordered','toolkit','from','macwarehouse','something','like','included','antistatic']"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL2asBlDli-z"
      },
      "source": [
        "my_work= mnb.predict(art)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dnHUUozwwiN"
      },
      "source": [
        "def train_demo(articles_per_tag):\n",
        "    def get_unique_values(new_list):\n",
        "        unique = []\n",
        "        for ele in new_list:\n",
        "            if ele not in unique:\n",
        "                unique.append(ele)\n",
        "        return unique\n",
        "\n",
        "    def word_counts(uni_wrds,docs):\n",
        "        word_counter=[]\n",
        "        for u in range(len(uni_wrds)):\n",
        "            cnt=0\n",
        "            for doc in range(len(docs)):\n",
        "                w_cnts= Counter(docs[doc])\n",
        "                cnt= cnt + w_cnts.get(uni_wrds[u],0)\n",
        "            word_counter.append(cnt)\n",
        "        return word_counter\n",
        "\n",
        "    uniq_labels= articles_per_tag.keys()\n",
        "    uniq_list=[]\n",
        "    lab_wise_cnts=[]\n",
        "\n",
        "    for ul in uniq_labels:\n",
        "        lab_uniq_list=[]\n",
        "        lab_data= articles_per_tag.get(ul)\n",
        "        lc=0\n",
        "        for d1 in range(len(lab_data)):\n",
        "            word_list=get_unique_values(lab_data[d1])\n",
        "            lab_uniq_list.extend(word_list)\n",
        "            lc= lc+len(lab_data[d1])\n",
        "        uniq_list.extend(lab_uniq_list)\n",
        "        lab_wise_cnts.append(lc)\n",
        "    uniq_list= get_unique_values(uniq_list) \n",
        "    \n",
        "    lab_docword_counts=[]\n",
        "    for doc_lab in uniq_labels:\n",
        "        doc_list= articles_per_tag.get(doc_lab)\n",
        "        dwc_dict= dict(zip(uniq_list,word_counts(uniq_list,doc_list)))\n",
        "        lab_docword_counts.append(dwc_dict)\n",
        "        \n",
        "    train_wrd_cnt= dict(zip(uniq_labels,lab_docword_counts))\n",
        "    train_lab_cnt= dict(zip(uniq_labels,lab_wise_cnts))\n",
        "\n",
        "    return train_wrd_cnt, train_lab_cnt       "
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaSdgz19sQRM"
      },
      "source": [
        "w_cnt,l_cnt= train_demo(input_data)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7zJXV92tR90"
      },
      "source": [
        "def predict_demo(data,results,labels):\n",
        "    \n",
        "    def probability(word, w_dict):\n",
        "        prob= math.log((w_dict.get(word,0)+1)/(sum(w_dict.values())+2))  \n",
        "        return prob\n",
        "\n",
        "    uniq_labels= labels.keys()\n",
        "    res=[]\n",
        "    for d in range(len(data)):\n",
        "        interim_res=[]\n",
        "        for label in uniq_labels:\n",
        "            p=0\n",
        "            for k in range(len(data[d])):\n",
        "                p= p + probability(data[d][k],results.get(label))\n",
        "            \n",
        "            fin_p= round(p + math.log(labels.get(label)/sum(labels.values())),4)\n",
        "            \n",
        "            interim_res.append(fin_p)\n",
        "        res.append(interim_res)\n",
        "    \n",
        "    results_dict=[]\n",
        "    for r in range(len(res)):\n",
        "        d= dict(zip(uniq_labels,res[r]))\n",
        "        results_dict.append(d)\n",
        "\n",
        "    return results_dict"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx353jETtZtV"
      },
      "source": [
        "s_res= predict(test_data,w_cnt,l_cnt)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UpbHdyf0HsD",
        "outputId": "e043a3a6-379c-4766-c904-0d4284ad42c4"
      },
      "source": [
        "len(s_res)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "735"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw3zVUoP0NtX"
      },
      "source": [
        "s_res[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ_XkTfJ6Nmg"
      },
      "source": [
        "max_key=[]\n",
        "for n in range(len(s_res)):\n",
        "    max_key.append(max(s_res[n], key=lambda key: s_res[n][key]))"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ojsWMWY8NTz",
        "outputId": "1d103b2a-1343-44d4-f877-47733c3f6658"
      },
      "source": [
        "max_key[20:30]"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['business',\n",
              " 'politics',\n",
              " 'sport',\n",
              " 'business',\n",
              " 'politics',\n",
              " 'sport',\n",
              " 'business',\n",
              " 'sport',\n",
              " 'sport',\n",
              " 'business']"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4Omv2u8PXQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}